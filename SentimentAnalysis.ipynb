{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "This file contains code on:\n",
        "- how YouTube Videos were searched and downloaded, as well as transcribed\n",
        "- how sentiment scores were generated (from X, Reddit posts + YouTube videos)\n",
        "\n"
      ],
      "metadata": {
        "id": "88-YEqE199lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collecting YouTube Videos"
      ],
      "metadata": {
        "id": "pgjjXlvd-BAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepping to download the videos as mp3\n",
        "!pip install yt-dlp\n",
        "!apt-get install ffmpeg\n",
        "!pip install thefuzz[speedup]"
      ],
      "metadata": {
        "id": "BVQ4lXlF-GTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('YT_API')\n",
        "SEARCH_QUERY = \"\\\"$AAPL\\\" | \\\"$MSFT\\\" | \\\"$NVDA\\\" | \\\"$AMZN\\\" | \\\"$META\\\" | \\\"$BRK.B\\\" | \\\"$GOOGL\\\" | \\\"$AVGO\\\" | \\\"$GOOG\\\" | \\\"$TSLA\\\"\"\n",
        "MAX_RESULTS_PER_CALL = 50\n",
        "TOTAL_RESULTS_NEEDED = 8000\n",
        "OUTPUT_FILE = \"youtube_shorts_results.json\"\n",
        "\n",
        "# Load existing data if file exists\n",
        "if os.path.exists(OUTPUT_FILE):\n",
        "    with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        all_items = json.load(f)\n",
        "else:\n",
        "    all_items = []\n",
        "\n",
        "existing_ids = set(item[\"id\"][\"videoId\"] for item in all_items if \"id\" in item and \"videoId\" in item[\"id\"])\n",
        "\n",
        "base_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "\n",
        "# Loop through years\n",
        "for year in range(2020, 2025):\n",
        "    if len(all_items) >= TOTAL_RESULTS_NEEDED:\n",
        "        break\n",
        "\n",
        "    start_date = f\"{year}-01-01T00:00:00Z\"\n",
        "    end_date = f\"{year}-12-31T23:59:59Z\"\n",
        "    next_page_token = None\n",
        "\n",
        "    print(f\"üìÖ Searching year: {year}\")\n",
        "\n",
        "    while True:\n",
        "        if len(all_items) >= TOTAL_RESULTS_NEEDED:\n",
        "            break\n",
        "\n",
        "        params = {\n",
        "            \"key\": API_KEY,\n",
        "            \"part\": \"snippet\",\n",
        "            \"q\": SEARCH_QUERY,\n",
        "            \"type\": \"video\",\n",
        "            \"videoDuration\": \"short\",\n",
        "            \"maxResults\": MAX_RESULTS_PER_CALL,\n",
        "            \"relevanceLanguage\": \"en\",\n",
        "            \"publishedAfter\": start_date,\n",
        "            \"publishedBefore\": end_date\n",
        "        }\n",
        "\n",
        "        if next_page_token:\n",
        "            params[\"pageToken\"] = next_page_token\n",
        "\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ö†Ô∏è Error {response.status_code}: {response.text}\")\n",
        "            break  # Stop this year if error occurs\n",
        "\n",
        "        data = response.json()\n",
        "        items = data.get(\"items\", [])\n",
        "        new_count = 0\n",
        "\n",
        "        for item in items:\n",
        "            video_id = item.get(\"id\", {}).get(\"videoId\")\n",
        "            if video_id and video_id not in existing_ids:\n",
        "                all_items.append(item)\n",
        "                existing_ids.add(video_id)\n",
        "                new_count += 1\n",
        "\n",
        "        print(f\"Collected: {len(all_items)} / {TOTAL_RESULTS_NEEDED} (Added {new_count} new)\")\n",
        "\n",
        "        next_page_token = data.get(\"nextPageToken\")\n",
        "        if not next_page_token:\n",
        "            break  # No more results for this year\n",
        "\n",
        "        # Save after every batch\n",
        "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(all_items, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"‚úÖ Finished collecting Shorts from 2005 to 2024.\")"
      ],
      "metadata": {
        "id": "Buf1DjHd98-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_eWMReoMHnb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "# Load JSON data\n",
        "with open(\"youtube_shorts_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Output CSV file\n",
        "with open(\"youtube_shorts_results.csv\", \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
        "    fieldnames = [\"publishTime\", \"title\", \"description\", \"video_url\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "\n",
        "    for item in data:\n",
        "        snippet = item.get(\"snippet\", {})\n",
        "        video_id = item.get(\"id\", {}).get(\"videoId\")\n",
        "\n",
        "        if not video_id:\n",
        "            continue\n",
        "\n",
        "        writer.writerow({\n",
        "            \"publishTime\": snippet.get(\"publishTime\", \"\"),\n",
        "            \"title\": snippet.get(\"title\", \"\"),\n",
        "            \"description\": snippet.get(\"description\", \"\"),\n",
        "            \"video_url\": f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "        })\n",
        "\n",
        "print(\"‚úÖ JSON data successfully converted to CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Files"
      ],
      "metadata": {
        "id": "8C2aqfL--UuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from thefuzz import fuzz\n",
        "\n",
        "# Paths and config\n",
        "CSV_FILE = \"/content/drive/MyDrive/FYP/filtered_file.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/FYP/mp3_audio\"\n",
        "TICKERS = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'META', 'BRK.B', 'GOOGL', 'AVGO', 'GOOG', 'TSLA', 'SPY', 'SPX']\n",
        "FUZZY_THRESHOLD = 90  # Matching score to skip similar titles\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def contains_ticker(text):\n",
        "    text = text.upper()\n",
        "    return any(ticker in text for ticker in TICKERS)\n",
        "\n",
        "# Get already downloaded base filenames (without extension)\n",
        "existing_titles = [Path(f).stem for f in os.listdir(OUTPUT_DIR) if f.endswith(\".mp3\")]\n",
        "\n",
        "# Load filtered CSV\n",
        "with open(CSV_FILE, \"r\", encoding=\"utf-8\") as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    rows = [row for row in reader if contains_ticker(row.get(\"title\", \"\"))]\n",
        "    links = [(row[\"video_url\"], row[\"title\"]) for row in rows]\n",
        "\n",
        "print(f\"\\nüéØ Found {len(links)} matching videos. Checking for fuzzy duplicates...\\n\")\n",
        "\n",
        "for i, (url, title) in enumerate(links):\n",
        "    print(f\"üéµ [{i+1}/{len(links)}] Checking: {title}\")\n",
        "\n",
        "    # Fuzzy match against existing MP3 titles\n",
        "    already_downloaded = any(fuzz.token_set_ratio(title, existing) >= FUZZY_THRESHOLD for existing in existing_titles)\n",
        "    if already_downloaded:\n",
        "        print(f\"‚úÖ Skipped (fuzzy match found): {title}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Predict the actual filename\n",
        "        result = subprocess.run(\n",
        "            [\"yt-dlp\", \"--get-filename\", \"--output\", \"%(title).80s.%(ext)s\", url],\n",
        "            check=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=15\n",
        "        )\n",
        "        predicted_filename = result.stdout.strip()\n",
        "        mp3_path = os.path.join(OUTPUT_DIR, Path(predicted_filename).with_suffix(\".mp3\"))\n",
        "\n",
        "        print(f\"‚¨áÔ∏è Downloading to: {mp3_path}\")\n",
        "        subprocess.run([\n",
        "            \"yt-dlp\",\n",
        "            \"--extract-audio\",\n",
        "            \"--audio-format\", \"mp3\",\n",
        "            \"--output\", f\"{OUTPUT_DIR}/%(title).80s.%(ext)s\",\n",
        "            url\n",
        "        ], check=True, timeout=60)\n",
        "\n",
        "        existing_titles.append(Path(mp3_path).stem)  # Add to avoid duplicates later\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"‚è±Ô∏è Timeout exceeded. Skipping: {url}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"‚ùå Download failed: {url}\")"
      ],
      "metadata": {
        "id": "uocg7nso-H9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcribing Audio\n",
        "Note: This was originally done in a Spyder environment.\n",
        "\n",
        "Audio transcription was done using WhisperX (https://github.com/m-bain/whisperX)"
      ],
      "metadata": {
        "id": "kpkX86eM-esG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whisperx"
      ],
      "metadata": {
        "id": "L28bW47c_HYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "import torch\n",
        "import time\n",
        "import mysecrets\n",
        "import pandas as pd\n",
        "import os\n",
        "import ffmpeg\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Config\n",
        "hf_token = mysecrets.hf_token\n",
        "device = \"cuda\"\n",
        "audio_folder = r\"G:\\My Drive\\FYP\\mp3_audio\"\n",
        "csv_file = r\"G:\\My Drive\\FYP\\transcript.csv\"\n",
        "metadata_csv = r\"G:\\My Drive\\FYP\\youtube_shorts_results.csv\"\n",
        "batch_size = 4\n",
        "compute_type = \"int8\"\n",
        "\n",
        "# Load metadata CSV\n",
        "metadata_df = pd.read_csv(metadata_csv)\n",
        "video_metadata = {\n",
        "    row[\"title\"]: {\n",
        "        \"date\": row[\"publishTime\"],\n",
        "        \"description\": row[\"description\"],\n",
        "        \"url\": row[\"video_url\"]\n",
        "    } for _, row in metadata_df.iterrows()\n",
        "}\n",
        "\n",
        "print(f\"Loaded metadata for {len(video_metadata)} videos.\")\n",
        "print(f\"Sample video titles: {list(video_metadata.keys())[:5]}\")\n",
        "\n",
        "# Load WhisperX model\n",
        "model = whisperx.load_model(\"small.en\", device, compute_type=compute_type)\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "    result = model.transcribe(audio, batch_size=batch_size)\n",
        "    segments = result[\"segments\"]\n",
        "    return \" \".join(segment[\"text\"] for segment in segments)\n",
        "\n",
        "def extract_tickers(title):\n",
        "    stock_list = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'META', 'BRK.B', 'GOOGL', 'AVGO', 'GOOG', 'TSLA']\n",
        "    found = [s for s in stock_list if s in title.upper()]\n",
        "    return \", \".join(found) if found else \"Unknown\"\n",
        "\n",
        "def update_sheet(audio_file, transcription_text):\n",
        "    title = os.path.splitext(audio_file)[0].strip().lower()\n",
        "\n",
        "    # Fuzzy match against metadata titles\n",
        "    best_match, score = process.extractOne(title, video_metadata.keys())\n",
        "    print(f\"üé¨ Audio File: {audio_file}\")\n",
        "    print(f\"üîç Fuzzy matched '{title}' to '{best_match}' with score {score}\")\n",
        "\n",
        "    if score < 90:\n",
        "        print(f\"‚ùå No good match found for: {title} (score: {score})\")\n",
        "        return\n",
        "\n",
        "    metadata = video_metadata[best_match]\n",
        "    date = metadata.get(\"date\")\n",
        "    video_url = metadata.get(\"url\")\n",
        "    description = metadata.get(\"description\")\n",
        "    tickers = extract_tickers(best_match)\n",
        "\n",
        "    new_row = {\n",
        "        \"title\": best_match,\n",
        "        \"transcription\": transcription_text,\n",
        "        \"date\": date,\n",
        "        \"video URL\": video_url,\n",
        "        \"description\": description,\n",
        "        \"tickers found\": tickers\n",
        "    }\n",
        "\n",
        "    columns = [\"title\", \"transcription\", \"date\", \"video URL\", \"description\", \"tickers found\"]\n",
        "    if os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úÖ Transcription added for '{best_match}' to CSV.\")\n",
        "\n",
        "# Load already processed titles from CSV\n",
        "if os.path.exists(csv_file):\n",
        "    processed_df = pd.read_csv(csv_file)\n",
        "    processed_titles = set(processed_df['title'].str.lower())\n",
        "else:\n",
        "    processed_titles = set()\n",
        "\n",
        "# Process each audio file\n",
        "for audio_file in os.listdir(audio_folder):\n",
        "    if not (audio_file.endswith(\".mp3\") or audio_file.endswith(\".wav\")):\n",
        "        continue\n",
        "\n",
        "    title_base = os.path.splitext(audio_file)[0].strip().lower()\n",
        "    if title_base in processed_titles:\n",
        "        print(f\"‚úÖ Skipping already processed file: {audio_file}\")\n",
        "        continue\n",
        "\n",
        "    audio_file_path = os.path.join(audio_folder, audio_file)\n",
        "    print(f\"üéß Processing {audio_file_path}\")\n",
        "\n",
        "    try:\n",
        "        transcription = transcribe_audio(audio_file_path)\n",
        "        update_sheet(audio_file, transcription)\n",
        "        processed_titles.add(title_base)  # Add to set to avoid duplicate processing during the run\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {audio_file}: {e}\")\n",
        "\n",
        "# Cleanup\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "del model"
      ],
      "metadata": {
        "id": "B0Nxubd1_Wjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "6teg48WH-Mk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "ItD2YpFDRkdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load FinBERT\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "labels = [\"negative\", \"neutral\", \"positive\"]"
      ],
      "metadata": {
        "id": "edD6lY08-kar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of tickers to filter by\n",
        "stocks = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'META', 'BRK.B', 'GOOGL', 'AVGO', 'GOOG', 'TSLA']"
      ],
      "metadata": {
        "id": "7lZODATUtoDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch process sentiment scoring\n",
        "def get_sentiments_batch(texts, batch_size=32, max_length=64):\n",
        "    sentiments = []\n",
        "    scores = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Sentiment Scoring\"):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch_texts.tolist(),\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = F.softmax(outputs.logits, dim=1)\n",
        "\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        batch_scores = torch.max(probs, dim=1).values\n",
        "\n",
        "        for pred, score in zip(preds, batch_scores):\n",
        "            sentiments.append(labels[pred])\n",
        "            scores.append(score.item())\n",
        "\n",
        "    return sentiments, scores"
      ],
      "metadata": {
        "id": "OWBogcgntpfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Scores without Audio Transcripts"
      ],
      "metadata": {
        "id": "LB5zu_VF-zJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# news articles\n",
        "news1 = pd.read_csv(\"/content/drive/MyDrive/FYP/sp500_news_290k_articles.csv\")\n",
        "news2 = pd.read_csv(\"/content/drive/MyDrive/FYP/stock_data_articles.csv\")\n",
        "\n",
        "news2 = news2.rename(columns={\n",
        "    'symbol': 'ticker',\n",
        "    'Publishdate': 'date',\n",
        "    'Title': 'headline'\n",
        "})\n",
        "news2['time'] = None\n",
        "news2 = news2[['ticker', 'date', 'headline', 'time']]\n",
        "news1 = news1[['ticker', 'date', 'headline', 'time']]\n",
        "\n",
        "# filter news data by tickers\n",
        "df_news = pd.concat([news1, news2], ignore_index=True)\n",
        "df_news = df_news[df_news['ticker'].isin(stocks)]\n",
        "df_news = df_news.dropna(subset=['headline', 'date']).reset_index(drop=True)\n",
        "df_news['source'] = 'news'\n",
        "df_news['date'] = pd.to_datetime(df_news['date'], format='mixed', errors='coerce').dt.date\n",
        "\n",
        "# load reddit data (2021-2025)\n",
        "ds_reddit = load_dataset(\"johntoro/Reddit-Stock-Sentiment\", split=\"train\")\n",
        "df_reddit = ds_reddit.to_pandas()\n",
        "\n",
        "# use a regex pattern from the tickers\n",
        "pattern = r'\\b(?:' + '|'.join(re.escape(ticker) for ticker in stocks) + r')\\b'\n",
        "\n",
        "# keep posts that contain the tickker\n",
        "df_reddit['headline'] = df_reddit['title'].fillna('') + \" \" + df_reddit['text'].fillna('')\n",
        "df_reddit['ticker'] = None  # Initially None, we will extract ticker later\n",
        "df_reddit['date'] = pd.to_datetime(df_reddit['datetime']).dt.date\n",
        "df_reddit['time'] = pd.to_datetime(df_reddit['datetime']).dt.time\n",
        "df_reddit['source'] = 'reddit'\n",
        "df_reddit = df_reddit[df_reddit['headline'].str.contains(pattern, case=False, na=False)]\n",
        "\n",
        "# extract tickers\n",
        "df_reddit['ticker'] = df_reddit['headline'].apply(lambda x: [ticker for ticker in stocks if ticker in x])\n",
        "\n",
        "df_reddit = df_reddit[['ticker', 'date', 'headline', 'time', 'source']].dropna(subset=['headline'])\n",
        "\n",
        "# loading other reddit dataset\n",
        "df_reddit2 = pd.read_csv('/content/drive/MyDrive/FYP/posts.csv')\n",
        "\n",
        "# Combine 'title' and 'selftext' to create the 'headline' for sentiment analysis\n",
        "df_reddit2['headline'] = df_reddit2['title'].fillna('') + \" \" + df_reddit2['selftext'].fillna('')\n",
        "df_reddit2['ticker'] = None  # Initially None, we will extract ticker later\n",
        "df_reddit2['date'] = pd.to_datetime(df_reddit2['created_utc'], unit='s').dt.date  # Convert from Unix timestamp\n",
        "df_reddit2['time'] = pd.to_datetime(df_reddit2['created_utc'], unit='s').dt.time\n",
        "df_reddit2['source'] = 'reddit2'\n",
        "\n",
        "# filter based on tickers\n",
        "df_reddit2 = df_reddit2[df_reddit2['headline'].str.contains(pattern, case=False, na=False)]\n",
        "\n",
        "# extract tickers\n",
        "df_reddit2['ticker'] = df_reddit2['headline'].apply(lambda x: [ticker for ticker in stocks if ticker in x])\n",
        "\n",
        "df_reddit2 = df_reddit2[['ticker', 'date', 'headline', 'time', 'source']].dropna(subset=['headline'])\n",
        "\n",
        "# load twitter dataset\n",
        "twitter_ds = load_dataset(\"mjw/stock_market_tweets\", split=\"train\")\n",
        "df_twt = twitter_ds.to_pandas()\n",
        "df_twt['headline'] = df_twt['body']\n",
        "df_twt['ticker'] = df_twt['ticker_symbol']\n",
        "df_twt['date'] = pd.to_datetime(df_twt['post_date'], format='mixed', errors='coerce').dt.date\n",
        "df_twt['time'] = pd.to_datetime(df_twt['post_date'], format='mixed', errors='coerce').dt.time\n",
        "df_twt['source'] = 'twitter'\n",
        "\n",
        "# filter Twitter data by tickers\n",
        "df_twt = df_twt[df_twt['ticker'].isin(stocks)]\n",
        "\n",
        "df_twt = df_twt[['ticker', 'date', 'headline', 'time', 'source']].dropna(subset=['headline', 'date'])\n",
        "\n",
        "# Ensure 'date' columns are in datetime format\n",
        "df_news['date'] = pd.to_datetime(df_news['date'], errors='coerce')\n",
        "df_reddit['date'] = pd.to_datetime(df_reddit['date'], errors='coerce')\n",
        "df_reddit2['date'] = pd.to_datetime(df_reddit2['date'], errors='coerce')\n",
        "df_twt['date'] = pd.to_datetime(df_twt['date'], errors='coerce')\n",
        "\n",
        "# üìÜ Date Range Filtering\n",
        "df_news = df_news[df_news['date'].dt.year.between(2010, 2024)]\n",
        "df_reddit = df_reddit[df_reddit['date'].dt.year.between(2010, 2024)]\n",
        "df_reddit2 = df_reddit2[df_reddit2['date'].dt.year.between(2010, 2024)]\n",
        "df_twt = df_twt[df_twt['date'].dt.year.between(2010, 2024)]\n",
        "\n",
        "# üß© Combine All and Filter by Date Range (2010-2024)\n",
        "df_all = pd.concat([df_news, df_reddit, df_reddit2, df_twt], ignore_index=True)\n",
        "df_all = df_all[df_all['date'].dt.year.between(2010, 2024)]\n",
        "\n",
        "# Print final counts\n",
        "print(f\"Total News Articles (2010-2024): {len(df_news)}\")\n",
        "print(f\"Total Reddit Posts (2010-2024): {len(df_reddit) + len(df_reddit2)}\")\n",
        "print(f\"Total Twitter Posts (2010-2024): {len(df_twt)}\")\n",
        "print(f\"Combined Total (2010-2024): {len(df_all)}\")\n",
        "\n",
        "# üîç Run Sentiment Scoring\n",
        "sentiments, scores = get_sentiments_batch(df_all['headline'])\n",
        "\n",
        "# Add results to DataFrame\n",
        "df_all['sentiment'] = sentiments\n",
        "df_all['sentiment_score'] = scores\n",
        "\n",
        "# üß† Compute daily aggregated sentiment score\n",
        "daily_sentiment = df_all[df_all['sentiment'].isin(['positive', 'negative'])].copy()\n",
        "daily_sentiment['adjusted_score'] = daily_sentiment.apply(\n",
        "    lambda row: row['sentiment_score'] if row['sentiment'] == 'positive' else -row['sentiment_score'], axis=1\n",
        ")\n",
        "daily_scores = daily_sentiment.groupby('date')['adjusted_score'].mean().reset_index()\n",
        "daily_scores.rename(columns={'adjusted_score': 'daily_sentiment_score'}, inplace=True)\n",
        "\n",
        "print(daily_scores.tail())\n",
        "\n",
        "# ‚úÖ Save outputs\n",
        "df_all.to_csv(\"/content/drive/MyDrive/FYP/all_sentiment_results.csv\", index=False)\n",
        "daily_scores.to_csv(\"/content/drive/MyDrive/FYP/daily_sentiment_scores.csv\", index=False)"
      ],
      "metadata": {
        "id": "xFkjZJxc-w-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Scores with Audio"
      ],
      "metadata": {
        "id": "gEJtFfaguKGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# load audio transcripts\n",
        "df_transcripts = pd.read_csv(\"/content/drive/MyDrive/FYP/transcript.csv\")\n",
        "\n",
        "# preprocess the transcripts data to match our structure\n",
        "df_transcripts = df_transcripts.rename(columns={\n",
        "    'tickers found': 'ticker',\n",
        "    'transcription': 'headline'\n",
        "})\n",
        "\n",
        "# convert date column to datetime\n",
        "df_transcripts['date'] = pd.to_datetime(df_transcripts['date'], errors='coerce').dt.date\n",
        "df_transcripts['time'] = None\n",
        "df_transcripts['source'] = 'transcript'\n",
        "\n",
        "# filtering by target tickers and valid dates\n",
        "df_transcripts = df_transcripts[df_transcripts['ticker'].isin(stocks)]\n",
        "df_transcripts = df_transcripts.dropna(subset=['headline', 'date']).reset_index(drop=True)\n",
        "\n",
        "# ensuring 'date' columns are in datetime format\n",
        "df_news['date'] = pd.to_datetime(df_news['date'], errors='coerce')\n",
        "df_reddit['date'] = pd.to_datetime(df_reddit['date'], errors='coerce')\n",
        "df_reddit2['date'] = pd.to_datetime(df_reddit2['date'], errors='coerce')\n",
        "df_twt['date'] = pd.to_datetime(df_twt['date'], errors='coerce')\n",
        "\n",
        "# filtering all by date (2010 to 2024)\n",
        "df_news = df_news[df_news['date'].dt.year.between(2010, 2024)]\n",
        "df_reddit = df_reddit[df_reddit['date'].dt.year.between(2010, 2024)]\n",
        "df_reddit2 = df_reddit2[df_reddit2['date'].dt.year.between(2010, 2024)]\n",
        "df_twt = df_twt[df_twt['date'].dt.year.between(2010, 2024)]\n",
        "\n",
        "# combining and filtering by date\n",
        "df_all = pd.concat([df_news, df_reddit, df_reddit2, df_twt], ignore_index=True)\n",
        "df_all = df_all[df_all['date'].dt.year.between(2010, 2024)]  # Apply final date range filter\n",
        "\n",
        "# checking dataset amount\n",
        "print(f\"Total News Articles (2010-2024): {len(df_news)}\")\n",
        "print(f\"Total Reddit Posts (2010-2024): {len(df_reddit) + len(df_reddit2)}\")\n",
        "print(f\"Total Twitter Posts (2010-2024): {len(df_twt)}\")\n",
        "print(f\"Total Transcripts (2010-2024): {len(df_transcripts)}\")\n",
        "print(f\"Combined Total (2010-2024): {len(df_all)}\")\n",
        "\n",
        "# generate sentiment score\n",
        "sentiments, scores = get_sentiments_batch(df_all['headline'])\n",
        "\n",
        "# add results to dataframe\n",
        "df_all['sentiment'] = sentiments\n",
        "df_all['sentiment_score'] = scores\n",
        "\n",
        "# computing sentiment scores\n",
        "daily_sentiment = df_all[df_all['sentiment'].isin(['positive', 'negative'])].copy()\n",
        "daily_sentiment['adjusted_score'] = daily_sentiment.apply(\n",
        "    lambda row: row['sentiment_score'] if row['sentiment'] == 'positive' else -row['sentiment_score'], axis=1\n",
        ")\n",
        "daily_scores = daily_sentiment.groupby('date')['adjusted_score'].mean().reset_index()\n",
        "daily_scores.rename(columns={'adjusted_score': 'daily_sentiment_score'}, inplace=True)\n",
        "\n",
        "print(daily_scores.tail())\n",
        "\n",
        "# saving outputs\n",
        "df_all.to_csv(\"/content/drive/MyDrive/FYP/all_sentiment_results_w_audio.csv\", index=False)\n",
        "daily_scores.to_csv(\"/content/drive/MyDrive/FYP/daily_sentiment_scores_w_audio.csv\", index=False)"
      ],
      "metadata": {
        "id": "-QjAET9l-oS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}